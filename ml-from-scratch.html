<!DOCTYPE html>
<html lang="en">
<head>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>

    <link rel="stylesheet" type="text/css" href="http://jarfa.github.io/theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="http://jarfa.github.io/theme/stylesheet/pygments.min.css">
  <link rel="stylesheet" type="text/css" href="http://jarfa.github.io/theme/stylesheet/font-awesome.min.css">





  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />

<meta name="author" content="Jonathan Arfa" />
<meta name="description" content="A self-lead refresher course in basic ML algorithms I'm in the process of implementing various machine learning algorithms from scratch and will write a short series of blogs posts about this project. This first post will be about what motivated this project and a bit about what I've ..." />
<meta name="keywords" content="Machine Learning">
<meta property="og:site_name" content="Machine Learning and Tacos"/>
<meta property="og:title" content="Machine Learning From Scratch - Pt. I"/>
<meta property="og:description" content="A self-lead refresher course in basic ML algorithms I'm in the process of implementing various machine learning algorithms from scratch and will write a short series of blogs posts about this project. This first post will be about what motivated this project and a bit about what I've ..."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="http://jarfa.github.io/ml-from-scratch.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2017-01-03 00:00:00-05:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="http://jarfa.github.io/author/jonathan-arfa.html">
<meta property="article:section" content="Blog"/>
<meta property="article:tag" content="Machine Learning"/>
<meta property="og:image" content="/images/halong_kayak.jpg">
  <title>Machine Learning and Tacos &ndash; Machine Learning From Scratch - Pt. I</title>
</head>
<body>
  <aside>
    <div>
      <a href="http://jarfa.github.io">
        <img src="/images/halong_kayak.jpg" alt="Machine Learning and Tacos" title="Machine Learning and Tacos">
      </a>
      <h1><a href="http://jarfa.github.io">Machine Learning and Tacos</a></h1>
      <p>What else?</p>
      <nav>
        <ul class="list">
          <li><a href="http://jarfa.github.io/pages/about.html#about">About</a></li>
        </ul>
      </nav>
      <ul class="social">
        <li><a class="sc-twitter" href="https://twitter.com/jonarfa" target="_blank"><i class="fa fa-twitter"></i></a></li>
        <li><a class="sc-linkedin" href="https://www.linkedin.com/in/jarfa" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        <li><a class="sc-github" href="https://github.com/jarfa/" target="_blank"><i class="fa fa-github"></i></a></li>
      </ul>
    </div>
  </aside>
  <main>

<article>
  <header>
    <h1 id="ml-from-scratch">Machine Learning From Scratch - Pt. I</h1>
    <p>Posted on Tue 03 January 2017 in <a href="http://jarfa.github.io/category/blog.html">Blog</a></p>
  </header>
  <div>
    <h2>A self-lead refresher course in basic ML algorithms</h2>
<p>I'm in the process of <a href="https://github.com/jarfa/ML_from_scratch">implementing various machine learning algorithms from scratch</a> and will write a short series of blogs posts about this project. This first post will be about what motivated this project and a bit about what I've learned. Further posts will compare my algorithms' performance vs. that of the algorithms in <a href="http://scikit-learn.org/stable/">scikit-learn</a>, and will attempt to walk the reader through how these algorithms work.</p>
<p>For now the algorithms include:</p>
<ul>
<li>
<p>Regression (logistic and least squares) via gradient descent</p>
</li>
<li>
<p>Decision Trees</p>
</li>
<li>
<p>Random Forests  </p>
</li>
</ul>
<!-- *  Gradient Boosted Trees -->

<p>I'll be benchmarking these algorithms on the <a href="http://scikit-learn.org/stable/datasets/index.html#optical-recognition-of-handwritten-digits-data-set">handwritten digits dataset</a> that ships with scikit-learn. The code as written only does binary classification, so for each digit 0-9 I'll define one digit as the target and the rest as non-targets.</p>
<hr />
<h2>Motivation</h2>
<p>This project was inspired by a hackathon in the Spring of 2016 when I was working on the engineering team at Magnetic. At Magnetic I had been working on our machine learning systems that used <a href="https://github.com/JohnLangford/vowpal_wabbit">Vowpal Wabbit</a>, and in this hackathon we implemented a similar logistic regression solver in the Go language - which we called Gowpal Wabbit (a large part of projects at Magnetic involved arguing about how to construct a witty/punny name, Gowpal Wabbit was not our most clever). My colleague Dan Crosta wrote a great blog post about <a href="https://late.am/post/2016/04/22/demystifying-logistic-regression.html">what he learned about logistic regression</a> from the process.</p>
<p>While I had learned a lot about the most commonly used algorithms in grad school and at work, writing logistic regression from scratch and teaching a team of software engineers the math and intuition beyond the gradient descent solver made me think much harder about the various choices that go into writing a working implementation. It was a surprisingly educational experience.</p>
<p>Since then I've changed jobs, and after a traveling a lot in the Summer and Fall of 2016 I've found some free time again. I am (intermittently) writing some of the more commonly used ML algorithms from near-scratch and comparing their performance (both in terms of predictive power and computational efficiency) versus scikit-learn.</p>
<p>Note: these algorithms exist for my re-education and very little else. My algorithms will hopefully be just as good at prediction as scikit-learn's options, but theirs are more fully-featured and are much faster (since they're written in <a href="http://cython.org/">Cython</a>). There is no reason anybody should be using my algorithms unless they find my code educational.</p>
<p>I had a few self-imposed rules for this project:</p>
<ul>
<li>
<p>Along with writing the machine learning algorithms, rewrite any necessary utility functions - within reason. For example, I used numpy instead of creating my own linear algebra library, but wrote from scratch performance metrics (LogLoss, ROC AUC), feature normalization, etc.</p>
</li>
<li>
<p>Don't just look at scikit-learn's code and copy what they do, I should implement things my way. But my algorithms' variable and method names are similar to theirs to simplify my demonstration code, and in some cases I looked through their code after my code was already working.</p>
</li>
<li>
<p>Make my code user-friendly - it should have a sane interface and readable code.</p>
</li>
<li>
<p>Don't go crazy - I'm not going to rewrite my algorithms in Cython or make the code <a href="http://pypy.org/">PyPy</a> compatible in order to match scikit-learn's speed. The value of this project to me is about making sure I know how to implement these algorithms efficiently, not about minimizing the actual time they require.</p>
</li>
</ul>
<h2>Lessons Learned</h2>
<p>Forcing yourself to rewrite from scratch algorithms you think you already know is full of fun discoveries. Things I learned include:</p>
<ul>
<li>
<p>You don't need to regularize the bias (a.k.a. intercept) term when fitting a regression model via gradient descent, especially when your input data is normalized (transformed so that the mean is 0 and the standard deviation is 1.) I learned this from colleagues of mine on the <a href="https://research.fb.com/category/facebook-ai-research-fair/">FAIR</a> team - it's pretty great to work in the same building as Machine Learning luminaries.</p>
</li>
<li>
<p>Something that should have been obvious beforehand - in gradient descent your learning rate and regularization rate should depend greatly on the minibatch size.</p>
</li>
<li>
<p>In an attempt to mimic Vowpal Wabbit's behavior, my first implementation of regression SGD used a hash table (the Python dictionary) to store the coefficients. While this allowed a lot of flexibility when dealing with sparse or categorical data, it resulted in a massive speed hit - especially when regularizing the coefficients. I could have made the hash table work well had I pre-allocated a large array of coefficients and hashed features for the user like VW does. For now I just require the user to turn their data into an array of floats before training the model, but a feature hasher would be easy to implement.</p>
</li>
<li>
<p>Writing a fast decision tree algorithm is hard. My implementation is orders of magnitude slower than scikit-learn's, and it could stand to be looked over carefully and optimized. Perhaps I'll go back and do that before I implement new algorithms.</p>
</li>
<li>
<p>Writing a <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">ROC AUC</a> implementation that runs in O(N^2) time is quite easy - but with a bit of thought, you can make it run in <a href="https://github.com/jarfa/ML_from_scratch/blob/12d91ee4109410855b09aa7da9df345ae79e117d/util.py#L40">O(N) time</a>.</p>
</li>
<li>
<p><a href="http://ggplot2.org/">R's ggplot2</a> package still has no peer among Python packages for graphics. For a while I've been doing most of my coding in Python, then moving my results to R for plotting with ggplot2. I wanted to use <a href="http://ggplot.yhathq.com/">Python's reimplmentation of ggplot</a> in this project to chart algorithm performance, but it's missing many crucial features and cannot yet replace R's version. <a href="http://seaborn.pydata.org/">Seaborn</a> is similar to ggplot2 in many ways, but is not quite as flexible. And while <a href="http://matplotlib.org/">matplotlib</a> is incredibly flexible, the the brevity and power of R's ggplot2 is far more appealing to me.</p>
</li>
</ul>
<!-- *  Gradient boosted trees sound so simple to implement - just iteratively build trees on the gradients! But there is apparently some some subtlety. My code for this algorithm will go on GitHub once I figure out what I'm doing wrong. -->
  </div>
  <div class="tag-cloud">
    <p>
      <a href="http://jarfa.github.io/tag/machine-learning.html">Machine Learning</a>
    </p>
  </div>
</article>

    <footer>
        <p>&copy; Jonathan Arfa </p>
<p>Built using <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a></p>    </footer>
  </main>

<!-- Google Analytics -->
<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-78236307-1', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->



<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "name": "Machine Learning From Scratch - Pt. I",
  "headline": "Machine Learning From Scratch - Pt. I",
  "datePublished": "2017-01-03 00:00:00-05:00",
  "dateModified": "",
  "author": {
    "@type": "Person",
    "name": "Jonathan Arfa",
    "url": "http://jarfa.github.io/author/jonathan-arfa.html"
  },
  "image": "/images/halong_kayak.jpg",
  "url": "http://jarfa.github.io/ml-from-scratch.html",
  "description": "A self-lead refresher course in basic ML algorithms I'm in the process of implementing various machine learning algorithms from scratch and will write a short series of blogs posts about this project. This first post will be about what motivated this project and a bit about what I've ..."
}
</script></body>
</html>